{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Module & Utility Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Utilities.EvaluationMain import *\n",
    "from Utilities.Utilities import ReadYaml, SerializeObjects, DeserializeObjects, LoadModelConfigs, LoadParams\n",
    "from Models.Caller64 import *\n",
    "from Utilities.Visualization import VisReconGivenZ_FCA, HeatMapFreqZ_FCA, VisReconGivenFC_ZA, VisReconExtractZ_FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading: Model Configs and Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_tables(directory, acc_keyword, acc_pattern, mi_keyword, mi_pattern):\n",
    "    \"\"\"Load and combine evaluation tables from directory based on filtering keywords.\"\"\"\n",
    "    table_list = os.listdir(directory)\n",
    "    \n",
    "    # Load accuracy tables\n",
    "    acc_list = [tab for tab in table_list if acc_keyword in tab and acc_pattern in tab]\n",
    "    acc_df = pd.DataFrame()\n",
    "    for tab in acc_list:\n",
    "        file_path = os.path.join(directory, tab)\n",
    "        df = pd.read_csv(file_path)\n",
    "        acc_df = pd.concat([acc_df, df], axis=0)\n",
    "    \n",
    "    # Compute RMSE from MSE\n",
    "    if 'MSEdenorm' in acc_df.columns:\n",
    "        acc_df['RMSE'] = np.sqrt(acc_df['MSEdenorm'])\n",
    "    \n",
    "    # Load MI tables\n",
    "    mi_list = [tab for tab in table_list if mi_keyword in tab and mi_pattern in tab]\n",
    "    mi_df = pd.DataFrame()\n",
    "    for tab in mi_list:\n",
    "        file_path = os.path.join(directory, tab)\n",
    "        df = pd.read_csv(file_path)\n",
    "        mi_df = pd.concat([mi_df, df], axis=0)\n",
    "    \n",
    "    return acc_df, mi_df\n",
    "\n",
    "def load_config_models(config_directory, include_keyword='Config', exclude_keyword='Eval', key='Models'):\n",
    "    \"\"\"Extract model configurations from YAML files.\"\"\"\n",
    "    config_files = [f for f in os.listdir(config_directory)\n",
    "                    if include_keyword in f and exclude_keyword not in f]\n",
    "    model_dict = {}\n",
    "    for config in config_files:\n",
    "        full_path = os.path.join(config_directory, config)\n",
    "        config_data = ReadYaml(full_path)\n",
    "        model_dict[config.split('.')[0]] = list(config_data.get(key, {}).keys())\n",
    "    return model_dict\n",
    "\n",
    "# Load evaluation tables\n",
    "eval_directory = './EvalResults/Tables/'\n",
    "AcctableSet, MItableSet = load_evaluation_tables(\n",
    "    eval_directory,\n",
    "    acc_keyword='Acc',\n",
    "    acc_pattern='Nj1_FC',\n",
    "    mi_keyword='MI',\n",
    "    mi_pattern='Nj1_FC')\n",
    "\n",
    "# Load benchmark tables\n",
    "bench_directory = './Benchmarks/EvalResults/Tables/'\n",
    "BenchAcctableSet, BenchMItableSet = load_evaluation_tables(\n",
    "    bench_directory,\n",
    "    acc_keyword='Acc',\n",
    "    acc_pattern='NjAll',\n",
    "    mi_keyword='MI',\n",
    "    mi_pattern='NjAll')\n",
    "\n",
    "# Unify metric naming conventions\n",
    "metrics_map = {\n",
    "    '(i) $I(V;\\\\acute{\\\\Theta} \\\\mid X)$': '(ii) $I(V;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$',\n",
    "    '(ii) $I(S;\\\\acute{\\\\Theta} \\\\mid X)$': '(iii) $I(S;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$'}\n",
    "\n",
    "BenchMItableSet['Metrics'] = BenchMItableSet['Metrics'].replace(metrics_map)\n",
    "MetricTypes = np.unique(MItableSet['MetricType']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_models(config_directory, include_keyword='Config', exclude_keyword='Eval', key='Models'):\n",
    "    \"\"\"Extract model configurations from YAML files.\"\"\"\n",
    "    config_files = [f for f in os.listdir(config_directory)\n",
    "                    if include_keyword in f and exclude_keyword not in f]\n",
    "    model_dict = {}\n",
    "    for config in config_files:\n",
    "        full_path = os.path.join(config_directory, config)\n",
    "        config_data = ReadYaml(full_path)\n",
    "        model_dict[config.split('.')[0]] = list(config_data.get(key, {}).keys())\n",
    "    return model_dict\n",
    "\n",
    "def prepare_analysis_table(mi_df, acc_df, target_models):\n",
    "    \"\"\"\n",
    "    Merge MI and accuracy data, compute composite ISCORE metrics, and parse model parameters.\n",
    "    \"\"\"\n",
    "    # Filter by target models\n",
    "    mi_table = mi_df[mi_df['Model'].isin(target_models)].reset_index(drop=True)\n",
    "    acc_table = acc_df[acc_df['Model'].isin(target_models)].reset_index(drop=True)\n",
    "    \n",
    "    # Prepare accuracy table\n",
    "    if 'MAPEnorm' in acc_table.columns:\n",
    "        acc_table['MAPEnorm'] = acc_table['MAPEnorm'] / 100\n",
    "    acc_table = acc_table[['Model', 'MeanKldRes', 'RMSE', 'R2denorm']].copy()\n",
    "    acc_table.columns = ['Model', 'FQI', 'RMSE', 'R2denorm']\n",
    "    \n",
    "    # Process MI table: group by Model and Metrics, then pivot\n",
    "    mi_grouped = mi_table.groupby(['Model', 'Metrics']).mean(numeric_only=True).reset_index()\n",
    "    mi_pivot = pd.pivot(mi_grouped, index='Model', columns='Metrics', values='Values').reset_index()\n",
    "    \n",
    "    # Merge tables\n",
    "    merged_table = pd.merge(mi_pivot, acc_table, on='Model', how='inner').sort_values('Model').reset_index(drop=True)\n",
    "    \n",
    "    # Parse model parameters from naming convention\n",
    "    split_cols = merged_table['Model'].str.split('_', expand=True)\n",
    "    if split_cols.shape[1] == 6:\n",
    "        split_cols.columns = ['Prefix', 'Type', 'Depth', 'LatDim', 'Comp', 'Source']\n",
    "        merged_table = pd.concat([merged_table, split_cols], axis=1)\n",
    "    elif split_cols.shape[1] == 4:\n",
    "        mask = split_cols[3].isna() | (split_cols[3] == 'None')\n",
    "        split_cols.loc[mask, 3] = split_cols.loc[mask, 2]\n",
    "        split_cols.loc[mask, 2] = 0\n",
    "        split_cols.columns = ['Prefix', 'Type', 'LatDim', 'Source']\n",
    "        merged_table = pd.concat([merged_table, split_cols], axis=1)\n",
    "    else:\n",
    "        print(\"Warning: Unexpected model naming format. Check the 'Model' column.\")\n",
    "    \n",
    "    # Compute individual performance metrics\n",
    "    ms_metric_col = '(i) $I(V; \\\\acute{Z} \\\\mid Z)$'\n",
    "    \n",
    "    merged_table['MP'] = 1-np.sqrt(1 - np.exp(-2 * merged_table['(ii) $I(V;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$']))\n",
    "    merged_table['AC'] = np.sqrt(1 - np.exp(-2 * merged_table['(iii) $I(S;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$']))\n",
    "    merged_table['SS'] = 1-np.sqrt(1 - np.exp(-2 * merged_table['FQI']))\n",
    "    merged_table['RA'] = merged_table['R2denorm']\n",
    "    merged_table['NMSE'] = 1 - merged_table['RA']\n",
    "\n",
    "    # Compute composite ISCORE (with/without MS metric)\n",
    "    if ms_metric_col in merged_table.columns:\n",
    "        mask_has_ms = merged_table[ms_metric_col].notna()\n",
    "        merged_table.loc[mask_has_ms, 'MS'] = np.sqrt(1 - np.exp(-2 * merged_table.loc[mask_has_ms, ms_metric_col]))\n",
    "        \n",
    "        # ISCORE with MS\n",
    "        merged_table.loc[mask_has_ms, 'ISCOREam'] = mean(merged_table.loc[mask_has_ms, ['MS', 'MP', 'AC', 'SS', 'RA']], kind=\"am\", axis=1)\n",
    "        merged_table.loc[mask_has_ms, 'ISCOREgm'] = mean(merged_table.loc[mask_has_ms, ['MS', 'MP', 'AC', 'SS', 'RA']], kind=\"gm\", axis=1)\n",
    "        merged_table.loc[mask_has_ms, 'ISCOREhm'] = mean(merged_table.loc[mask_has_ms, ['MS', 'MP', 'AC', 'SS', 'RA']], kind=\"hm\", axis=1)\n",
    "        \n",
    "        # ISCORE without MS\n",
    "        mask_no_ms = merged_table[ms_metric_col].isna()\n",
    "        merged_table.loc[mask_no_ms, 'ISCOREam'] = mean(merged_table.loc[mask_no_ms, ['MP', 'AC', 'SS', 'RA']], kind=\"am\", axis=1)\n",
    "        merged_table.loc[mask_no_ms, 'ISCOREgm'] = mean(merged_table.loc[mask_no_ms, ['MP', 'AC', 'SS', 'RA']], kind=\"gm\", axis=1)\n",
    "        merged_table.loc[mask_no_ms, 'ISCOREhm'] = mean(merged_table.loc[mask_no_ms, ['MP', 'AC', 'SS', 'RA']], kind=\"hm\", axis=1)\n",
    "    else:\n",
    "        merged_table['ISCOREam'] = mean(merged_table[['MP', 'AC', 'SS', 'RA']], kind=\"am\", axis=1)\n",
    "        merged_table['ISCOREgm'] = mean(merged_table[['MP', 'AC', 'SS', 'RA']], kind=\"gm\", axis=1)\n",
    "        merged_table['ISCOREhm'] = mean(merged_table[['MP', 'AC', 'SS', 'RA']], kind=\"hm\", axis=1)\n",
    "        \n",
    "    return merged_table\n",
    "\n",
    "def amean(x, weights=None, axis=None, eps=None):\n",
    "    \"\"\"Arithmetic mean (simple or weighted).\"\"\"\n",
    "    if isinstance(x, (pd.DataFrame, pd.Series)):\n",
    "        x = x.values\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    if weights is None:\n",
    "        return np.mean(x, axis=axis)\n",
    "\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    return np.average(x, weights=w, axis=axis)\n",
    "    \n",
    "def gmean(x, weights=None, axis=None, eps=1e-12):\n",
    "    \"\"\"Geometric mean with eps stabilization for zero values.\"\"\"\n",
    "    if isinstance(x, (pd.DataFrame, pd.Series)):\n",
    "        x = x.values\n",
    "    \n",
    "    x = np.asarray(x, dtype=float)\n",
    "    logs = np.log(x + eps)\n",
    "    if weights is None:\n",
    "        return np.exp(np.mean(logs, axis=axis))\n",
    "    return np.exp(np.average(logs, weights=np.asarray(weights, dtype=float), axis=axis))\n",
    "\n",
    "def hmean(x, weights=None, axis=None, eps=1e-12):\n",
    "    \"\"\"Harmonic mean with eps stabilization.\"\"\"\n",
    "    if isinstance(x, (pd.DataFrame, pd.Series)):\n",
    "        x = x.values\n",
    "    \n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if weights is None:\n",
    "        denom = np.sum(1.0 / (x + eps), axis=axis)\n",
    "        count = x.size if axis is None else x.shape[axis]\n",
    "        return count / denom\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    return np.sum(w, axis=axis) / np.sum(w / (x + eps), axis=axis)\n",
    "\n",
    "def mean(x, kind=\"gm\", weights=None, axis=None, eps=1e-12):\n",
    "    \"\"\"Unified interface for arithmetic, geometric, and harmonic means.\"\"\"\n",
    "    kind = kind.lower()\n",
    "    if kind == \"am\":\n",
    "        return amean(x, weights=weights, axis=axis)\n",
    "    if kind == \"gm\":\n",
    "        return gmean(x, weights=weights, axis=axis, eps=eps)\n",
    "    if kind == \"hm\":\n",
    "        return hmean(x, weights=weights, axis=axis, eps=eps)\n",
    "    raise ValueError(\"kind must be one of {'am','gm','hm'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Table Construction and Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration models and combine all model keys\n",
    "config_directory = './Config/'\n",
    "TabLists = load_config_models(config_directory)\n",
    "AnalTabList = list(np.concatenate([tabs for key, tabs in TabLists.items()]))\n",
    "\n",
    "legend_map = {\n",
    "    'Depth': r'$\\zeta$',\n",
    "    'LatDim': r'$J$',\n",
    "    'Comp': r'$C$'\n",
    "}\n",
    "\n",
    "# Define main models (excluded from ablation analysis)\n",
    "MainList = [\n",
    "    'SKZFC_ART_1_30_800_Mimic',\n",
    "    'SKZFC_ART_1_30_800_VitalDB',\n",
    "    'SKZFC_II_1_30_800_Mimic',\n",
    "    'SKZFC_II_1_30_800_VitalDB'\n",
    "]\n",
    "\n",
    "AnalMetricList = [\n",
    "    '(i) $I(V; \\\\acute{Z} \\\\mid Z)$',\n",
    "    '(ii) $I(V;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$',\n",
    "    '(iii) $I(S;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$'\n",
    "]\n",
    "\n",
    "# Prepare merged analysis table by metric type\n",
    "AnalAccMItableDic = {}\n",
    "SubAcctableSet = AcctableSet.copy()\n",
    "for mtype in MetricTypes:\n",
    "    SubMItableSet = MItableSet[MItableSet['MetricType'] == mtype].reset_index(drop=True)\n",
    "    KldCols = [col for col in AcctableSet.columns if 'MeanKld' in col]\n",
    "    SelKldCols = [type for type in KldCols if mtype in type]\n",
    "    SubAcctableSet['MeanKldRes'] = AcctableSet[SelKldCols]\n",
    "    AnalAccMItableDic[mtype] = prepare_analysis_table(SubMItableSet, SubAcctableSet, AnalTabList)\n",
    "\n",
    "AnalAccMItableMerged = pd.concat(AnalAccMItableDic, axis=0)\n",
    "AnalAccMItableMerged = AnalAccMItableMerged.reset_index(level=0).rename(columns={'level_0': 'MetricType'})\n",
    "\n",
    "# Create sensitivity analysis table (exclude main models, keep only SKZFC variants)\n",
    "SenseAccMItable = AnalAccMItableMerged[~AnalAccMItableMerged['Model'].isin(MainList)].copy()\n",
    "SenseAccMItable = SenseAccMItable[SenseAccMItable['Model'].str.contains('SKZFC', na=False)].reset_index(drop=True)\n",
    "SenseAccMItable = SenseAccMItable[['MetricType','Model', 'Source', 'Type', 'Depth', 'LatDim', 'Comp', 'MS', 'MP', 'AC', 'SS', 'RA', 'ISCOREam', 'ISCOREgm', 'ISCOREhm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Analysis Table Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark configuration models\n",
    "Bench_config_directory = './Benchmarks/Config/'\n",
    "BenchTabLists = load_config_models(Bench_config_directory)\n",
    "BenchAnalTabList = list(np.concatenate([tabs for key, tabs in BenchTabLists.items()]))\n",
    "\n",
    "# Prepare benchmark analysis table by metric type\n",
    "BenchAnalAccMItableDic = {}\n",
    "SubBenchAcctableSet = BenchAcctableSet.copy()\n",
    "for mtype in MetricTypes:\n",
    "    SubBenchMItableSet = BenchMItableSet[BenchMItableSet['MetricType'] == mtype].reset_index(drop=True)\n",
    "    KldCols = [col for col in BenchAcctableSet.columns if 'MeanKld' in col]\n",
    "    SelKldCols = [type for type in KldCols if mtype in type]\n",
    "    SubBenchAcctableSet['MeanKldRes'] = BenchAcctableSet[SelKldCols]\n",
    "    BenchAnalAccMItableDic[mtype] = prepare_analysis_table(SubBenchMItableSet, SubBenchAcctableSet,  BenchAnalTabList)\n",
    "\n",
    "BenchAnalAccMItableMerged = pd.concat(BenchAnalAccMItableDic)\n",
    "BenchAnalAccMItableMerged = BenchAnalAccMItableMerged.reset_index(level=0).rename(columns={'level_0': 'MetricType'})\n",
    "\n",
    "# Extract reference performance baselines\n",
    "RAReferences = BenchAnalAccMItableMerged[['Type','Source', 'RA']].groupby(['Type','Source']).min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Aggregation and Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate performance across all methods\n",
    "AllMethods = pd.concat(AnalAccMItableDic.values(), ignore_index=True)\n",
    "\n",
    "# Keep SKZFC models only\n",
    "SKZFCs = AllMethods[AllMethods['Model'].str.contains('SKZFC', na=False)].reset_index(drop=True)\n",
    "\n",
    "# Filter out models below reference performance threshold\n",
    "RemoveList = []\n",
    "for idx, row in SKZFCs.iterrows():\n",
    "    target_mask = (RAReferences['Type'] == row['Type']) & (RAReferences['Source'] == row['Source'])\n",
    "    if row['RA'] < RAReferences[target_mask]['RA'].values[0]:\n",
    "        RemoveList.append(row['Model'])\n",
    "RemoveList = list(set(RemoveList))\n",
    "\n",
    "# Compute mean ISCOREgm for each (Source, Type, Model)\n",
    "group_cols = ['Source', 'Type', 'Model']\n",
    "MetricType = ['ISCOREgm']\n",
    "AggregatedScores = SKZFCs.groupby(group_cols)[MetricType].mean().reset_index()\n",
    "\n",
    "# Exclude underperforming models\n",
    "AggregatedScores_best = AggregatedScores[~AggregatedScores['Model'].isin(RemoveList)]\n",
    "\n",
    "n_methods = len(AnalAccMItableDic)\n",
    "n_metrics = len(MetricType)\n",
    "\n",
    "# Select best model per (Type, Source)\n",
    "best_model_indices = AggregatedScores_best.groupby(['Type', 'Source'])['ISCOREgm'].idxmax()\n",
    "BestOverallModels = AggregatedScores_best.loc[best_model_indices, ['Type', 'Source', 'Model', 'ISCOREgm']].reset_index(drop=True)\n",
    "BestModels = pd.merge(BestOverallModels[['Model']], AnalAccMItableMerged, on='Model').reset_index(drop=True)\n",
    "\n",
    "# Unified sensitivity analysis function\n",
    "def calculate_sensitivity(df, hp, score_col, group_by=('Type','Source'),\n",
    "                          latex_labels=True, legend_map=None,\n",
    "                          n_methods=None, n_metrics=None):\n",
    "    \"\"\"Compute sensitivity statistics for a given hyperparameter with LaTeX-ready labels.\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    if hp not in df.columns:\n",
    "        return None, None\n",
    "\n",
    "    legend_map = legend_map or {}\n",
    "\n",
    "    # Detailed statistics per (group_by..., hp)\n",
    "    group_cols_detail = list(group_by) + [hp]\n",
    "    detail_stats = (\n",
    "        df.groupby(group_cols_detail)[score_col]\n",
    "          .agg(mean='mean', std='std', max='max', min='min', n='size')\n",
    "          .reset_index()\n",
    "          .rename(columns={hp: 'Setting'})\n",
    "    )\n",
    "\n",
    "    # Build LaTeX display label for hyperparameter\n",
    "    if latex_labels:\n",
    "        if hp in legend_map and isinstance(legend_map[hp], str) and legend_map[hp].strip():\n",
    "            hp_label = legend_map[hp]\n",
    "        else:\n",
    "            fallback = {'Depth': r'$\\zeta$', 'LatDim': r'$J$', 'Comp': r'$C$'}\n",
    "            hp_label = fallback.get(hp, hp)\n",
    "            if isinstance(hp_label, str) and not (hp_label.startswith('$') and hp_label.endswith('$')):\n",
    "                hp_label = rf'$\\mathrm{{{hp_label}}}$'\n",
    "    else:\n",
    "        hp_label = legend_map.get(hp, hp)\n",
    "\n",
    "    detail_stats['Hyperparameter'] = hp_label\n",
    "\n",
    "    # Annotate effective data points\n",
    "    if (n_methods is not None) and (n_metrics is not None):\n",
    "        total_points = n_methods * n_metrics\n",
    "        detail_stats['n_effective'] = detail_stats['n'] * total_points\n",
    "        detail_stats['note'] = f'Each model aggregated from {n_methods}x{n_metrics}={total_points} data points'\n",
    "\n",
    "    # Summary statistics per (group_by..., Hyperparameter)\n",
    "    group_cols_summary = list(group_by) + ['Hyperparameter']\n",
    "    summary_stats = (\n",
    "        detail_stats.groupby(group_cols_summary)\n",
    "        .agg(\n",
    "            SensRange=('mean', lambda x: x.max() - x.min()),\n",
    "            SensStd=('mean', 'std'),\n",
    "            N_Settings=('Setting', 'nunique'),\n",
    "            Total_Models=('n', 'sum')\n",
    "        )\n",
    "        .reset_index()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    if 'n_effective' in detail_stats.columns:\n",
    "        tmp = (detail_stats.groupby(group_cols_summary)['n_effective']\n",
    "               .sum().reset_index(name='Total_Effective_DataPoints'))\n",
    "        summary_stats = summary_stats.merge(tmp, on=group_cols_summary, how='left')\n",
    "\n",
    "    # Column ordering\n",
    "    summary_cols = list(group_by) + ['Hyperparameter', 'SensStd', 'SensRange', 'N_Settings', 'Total_Models']\n",
    "    if 'Total_Effective_DataPoints' in summary_stats.columns:\n",
    "        summary_cols.append('Total_Effective_DataPoints')\n",
    "\n",
    "    detail_cols = list(group_by) + ['Hyperparameter', 'Setting', 'mean', 'std', 'max', 'min', 'n']\n",
    "    if 'n_effective' in detail_stats.columns:\n",
    "        detail_cols += ['n_effective', 'note']\n",
    "\n",
    "    return summary_stats[summary_cols], detail_stats[detail_cols]\n",
    "\n",
    "# Aggregated sensitivity analysis\n",
    "ParamCols = ['Depth', 'LatDim', 'Comp']\n",
    "\n",
    "# Merge aggregated scores with hyperparameter columns\n",
    "template_df = next(iter(AnalAccMItableDic.values()))\n",
    "merge_cols = ['Source', 'Type', 'Model'] + ParamCols\n",
    "template_df = template_df[template_df['Model'].str.contains('SKZFC', na=False)].reset_index(drop=True)\n",
    "template_subset = template_df.drop_duplicates(subset=merge_cols)[merge_cols]\n",
    "\n",
    "df_with_agg = pd.merge(AggregatedScores, template_subset, on=['Source', 'Type', 'Model'], how='left')\n",
    "\n",
    "# Run sensitivity analysis for each hyperparameter\n",
    "summary_list, detail_list = [], []\n",
    "for hp in ParamCols:\n",
    "    summary, detail = calculate_sensitivity(df_with_agg, hp, 'ISCOREgm', n_methods=n_methods, n_metrics=n_metrics)\n",
    "    if summary is not None:\n",
    "        summary_list.append(summary)\n",
    "        detail_list.append(detail)\n",
    "\n",
    "AggSensitivityDic = pd.concat(summary_list, ignore_index=True) if summary_list else pd.DataFrame()\n",
    "AggSensitivityDetailDic = pd.concat(detail_list, ignore_index=True) if detail_list else pd.DataFrame()\n",
    "\n",
    "# Individual method sensitivity analysis\n",
    "MetricTypesKey = ['fft', 'matching_pursuit', 'welch_evo']\n",
    "IScoreKey = ['ISCOREam', 'ISCOREgm', 'ISCOREhm']\n",
    "\n",
    "SensitivityDic, SensitivityDetailDic = {}, {}\n",
    "\n",
    "for mtype in MetricTypesKey:\n",
    "    df = AnalAccMItableDic[mtype][AnalAccMItableDic[mtype]['Model'].str.contains('SKZFC', na=False)]\n",
    "\n",
    "    SensitivityDic[mtype] = {}\n",
    "    SensitivityDetailDic[mtype] = {}\n",
    "\n",
    "    for iscore_col in IScoreKey:\n",
    "        summary_list, detail_list = [], []\n",
    "        for hp in ParamCols:\n",
    "            summary, detail = calculate_sensitivity(df, hp, iscore_col)\n",
    "            if summary is not None:\n",
    "                summary_list.append(summary)\n",
    "                detail_list.append(detail)\n",
    "\n",
    "        SensitivityDic[mtype][iscore_col] = pd.concat(summary_list, ignore_index=True) if summary_list else pd.DataFrame()\n",
    "        SensitivityDetailDic[mtype][iscore_col] = pd.concat(detail_list, ignore_index=True) if detail_list else pd.DataFrame()\n",
    "\n",
    "# Combine detailed sensitivity results\n",
    "SensitivityDetailTabs = pd.DataFrame()\n",
    "for idx, values in SensitivityDetailDic.items():\n",
    "    SubTab = pd.DataFrame()\n",
    "    for sub_idx, sub_values in values.items():\n",
    "        sub_values['IscoreType'] = sub_idx\n",
    "        SubTab = pd.concat([SubTab, sub_values])\n",
    "    SubTab['MetricType'] = idx\n",
    "    SensitivityDetailTabs = pd.concat([SensitivityDetailTabs, SubTab])\n",
    "    \n",
    "# =========================\n",
    "# Output Structure Documentation\n",
    "# =========================\n",
    "# AggSensitivityDic:\n",
    "#   Summary of hyperparameter sensitivity using the aggregated metric across all methods/scores.\n",
    "#   - Total_Models: number of unique models accumulated across settings for the hyperparameter.\n",
    "#   - Total_Effective_DataPoints: Total_Models × n_methods × n_metrics (reflects aggregation basis).\n",
    "#   Provides a single consolidated view of sensitivity.\n",
    "\n",
    "# AggSensitivityDetailDic:\n",
    "#   Detailed per-setting statistics using the aggregated metric.\n",
    "#   - n: number of unique models per (Type, Setting) for this specific hyperparameter.\n",
    "#   - n_effective: n × n_methods × n_metrics (effective points behind ISCOREgm).\n",
    "#   - note: human-readable explanation of aggregation basis.\n",
    "#   Shows how each setting behaves for the overall aggregated score.\n",
    "\n",
    "# SensitivityDic[mtype][iscore]:\n",
    "#   Summary sensitivity per method (mtype) and individual score (iscore).\n",
    "\n",
    "# SensitivityDetailDic[mtype][iscore]:\n",
    "#   Per-setting detailed stats per method (mtype) and score (iscore).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ablation model names from best models\n",
    "AblationList = []\n",
    "for Model in BestModels['Model'].unique():\n",
    "    splitted = Model.split('_')\n",
    "    splitted[0] = 'SKZ'\n",
    "    AblationList.append(\"_\".join(splitted))\n",
    "    splitted[0] = 'FC'\n",
    "    AblationList.append(\"_\".join(splitted))\n",
    "\n",
    "# Extract main models with fixed hyperparameters\n",
    "OnlyMainModels = AnalAccMItableMerged[(AnalAccMItableMerged['Model'].str.contains('SKZFC', na=False)) \n",
    "                                     & (AnalAccMItableMerged['Depth']=='1') \n",
    "                                     & (AnalAccMItableMerged['Comp']=='800')\n",
    "                                     & (AnalAccMItableMerged['LatDim']=='30')]\n",
    "\n",
    "# Extract ablation models\n",
    "OnlyAblModels = AnalAccMItableMerged[AnalAccMItableMerged['Model'].isin(AblationList)]\n",
    "OnlyAblModels = OnlyAblModels[['MetricType','Model', 'Source', 'Type', 'Depth', 'LatDim', 'Comp', \n",
    "                               'MS', 'MP', 'AC', 'SS', 'RA', 'ISCOREam',\t'ISCOREgm',\t'ISCOREhm']].reset_index(drop=True)\n",
    "\n",
    "# Create benchmark comparison table\n",
    "BenchCompTabs = pd.concat([OnlyMainModels, BenchAnalAccMItableMerged]).reset_index(drop=True)\n",
    "BenchCompTabs = BenchCompTabs[['MetricType','Model', 'Source', 'Type', 'Depth', 'LatDim', 'Comp', \n",
    "                               'MS', 'MP', 'AC', 'SS', 'RA', 'ISCOREam',\t'ISCOREgm',\t'ISCOREhm']].reset_index(drop=True)\n",
    "\n",
    "# Create ablation study table\n",
    "MainForAbl =  BestModels[['MetricType','Model', 'Source', 'Type', 'Depth', 'LatDim', 'Comp', \n",
    "            'MS', 'MP', 'AC', 'SS', 'RA', 'ISCOREam', 'ISCOREgm', 'ISCOREhm']]\n",
    "AblCompTabs = pd.concat([MainForAbl, OnlyAblModels]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BenchCompTabs.to_csv('EvalResults/SummaryTables/BenchCompTabs.csv', index=False) \n",
    "AblCompTabs.to_csv('EvalResults/SummaryTables/AblCompTabs.csv', index=False) \n",
    "AggSensitivityDetailDic.to_csv('EvalResults/SummaryTables/AggSensitivityDetail.csv', index=False) \n",
    "SensitivityDetailTabs.to_csv('EvalResults/SummaryTables/SensitivityDetailTabs.csv', index=False) \n",
    "SenseAccMItable.to_csv('EvalResults/SummaryTables/SenseAccMItable.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
