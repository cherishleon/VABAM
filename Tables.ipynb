{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Module & Utility Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Utilities.EvaluationMain import *\n",
    "from Utilities.Utilities import ReadYaml, SerializeObjects, DeserializeObjects, LoadModelConfigs, LoadParams\n",
    "from Models.Caller64 import *\n",
    "from Utilities.Visualization import VisReconGivenZ_FCA, HeatMapFreqZ_FCA, VisReconGivenFC_ZA, VisReconExtractZ_FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Configurations and Evaluation Tables (Accuracy & MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_tables(directory, acc_keyword, acc_pattern, mi_keyword, mi_pattern):\n",
    "    \"\"\"\n",
    "    Load and combine evaluation tables from a specified directory based on filtering keywords.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing CSV table files.\n",
    "        acc_keyword (str): Keyword to identify accuracy tables.\n",
    "        acc_pattern (str): Additional substring that accuracy table filenames must contain.\n",
    "        mi_keyword (str): Keyword to identify MI (Mutual Information) tables.\n",
    "        mi_pattern (str): Additional substring that MI table filenames must contain.\n",
    "\n",
    "    Returns:\n",
    "        acc_df (DataFrame): A concatenated DataFrame of accuracy tables with an added 'RMSE' column.\n",
    "        mi_df (DataFrame): A concatenated DataFrame of MI tables.\n",
    "    \"\"\"\n",
    "    # List all files in the specified directory\n",
    "    table_list = os.listdir(directory)\n",
    "    \n",
    "    # Load and combine accuracy tables\n",
    "    acc_list = [tab for tab in table_list if acc_keyword in tab and acc_pattern in tab]\n",
    "    acc_df = pd.DataFrame()\n",
    "    for tab in acc_list:\n",
    "        file_path = os.path.join(directory, tab)\n",
    "        df = pd.read_csv(file_path)\n",
    "        acc_df = pd.concat([acc_df, df], axis=0)\n",
    "    # Compute RMSE if the 'MSEdenorm' column is available\n",
    "    if 'MSEdenorm' in acc_df.columns:\n",
    "        acc_df['RMSE'] = np.sqrt(acc_df['MSEdenorm'])\n",
    "    \n",
    "    # Load and combine MI tables\n",
    "    mi_list = [tab for tab in table_list if mi_keyword in tab and mi_pattern in tab]\n",
    "    mi_df = pd.DataFrame()\n",
    "    for tab in mi_list:\n",
    "        file_path = os.path.join(directory, tab)\n",
    "        df = pd.read_csv(file_path)\n",
    "        mi_df = pd.concat([mi_df, df], axis=0)\n",
    "    \n",
    "    return acc_df, mi_df\n",
    "\n",
    "\n",
    "def load_config_models(config_directory, include_keyword='Config', exclude_keyword='Eval', key='Models'):\n",
    "    \"\"\"\n",
    "    Load configuration files from the specified directory and extract model keys.\n",
    "\n",
    "    Parameters:\n",
    "        config_directory (str): Path to the directory containing YAML configuration files.\n",
    "        include_keyword (str): Only consider files that include this keyword.\n",
    "        exclude_keyword (str): Exclude files that contain this keyword.\n",
    "        key (str): The key in the YAML file from which to extract model definitions.\n",
    "\n",
    "    Returns:\n",
    "        model_dict (dict): A dictionary mapping configuration file names (without extension)\n",
    "                           to a list of model keys.\n",
    "    \"\"\"\n",
    "    config_files = [f for f in os.listdir(config_directory)\n",
    "                    if include_keyword in f and exclude_keyword not in f]\n",
    "    model_dict = {}\n",
    "    for config in config_files:\n",
    "        full_path = os.path.join(config_directory, config)\n",
    "        config_data = ReadYaml(full_path)\n",
    "        model_dict[config.split('.')[0]] = list(config_data.get(key, {}).keys())\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main evaluation tables\n",
    "eval_directory = './EvalResults/Tables/'\n",
    "AcctableSet, MItableSet = load_evaluation_tables(\n",
    "    eval_directory,\n",
    "    acc_keyword='Acc',\n",
    "    acc_pattern='Nj1_FC',\n",
    "    mi_keyword='MI',\n",
    "    mi_pattern='Nj1_FC')\n",
    "\n",
    "\n",
    "# Benchmark evaluation tables\n",
    "bench_directory = './Benchmarks/EvalResults/Tables/'\n",
    "BenchAcctableSet, BenchMItableSet = load_evaluation_tables(\n",
    "    bench_directory,\n",
    "    acc_keyword='Acc',\n",
    "    acc_pattern='NjAll',\n",
    "    mi_keyword='MI',\n",
    "    mi_pattern='NjAll')\n",
    "\n",
    "# Define a mapping for metrics to be unified\n",
    "metrics_map = {\n",
    "    '(i) $I(V;\\\\acute{\\\\Theta} \\\\mid X)$': '(ii) $I(V;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$',\n",
    "    '(ii) $I(S;\\\\acute{\\\\Theta} \\\\mid X)$': '(iii) $I(S;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$'}\n",
    "\n",
    "# Create a new column ('UnifiedMetric') while preserving the original 'Metrics'\n",
    "BenchMItableSet['Metrics'] = BenchMItableSet['Metrics'].replace(metrics_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Construct Analysis Table and Perform ISCORE-Based Parameter Selection for Main Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return np.log1p(np.exp(x))  # numerically stable version of log(1 + exp(x))\n",
    "\n",
    "def load_config_models(config_directory, include_keyword='Config', exclude_keyword='Eval', key='Models'):\n",
    "    \"\"\"\n",
    "    Load configuration files from the specified directory and extract model keys.\n",
    "\n",
    "    Parameters:\n",
    "        config_directory (str): Path to the directory containing YAML configuration files.\n",
    "        include_keyword (str): Only consider files that include this keyword.\n",
    "        exclude_keyword (str): Exclude files that contain this keyword.\n",
    "        key (str): The key in the YAML file from which to extract model definitions.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping configuration file names (without extension)\n",
    "              to a list of model keys.\n",
    "    \"\"\"\n",
    "    config_files = [f for f in os.listdir(config_directory)\n",
    "                    if include_keyword in f and exclude_keyword not in f]\n",
    "    model_dict = {}\n",
    "    for config in config_files:\n",
    "        full_path = os.path.join(config_directory, config)\n",
    "        config_data = ReadYaml(full_path)\n",
    "        model_dict[config.split('.')[0]] = list(config_data.get(key, {}).keys())\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "def prepare_analysis_table(mi_df, acc_df, target_models, mi_metrics):\n",
    "    \"\"\"\n",
    "    Prepare the analysis table by merging MI and accuracy data, filtering by target models and metrics,\n",
    "    computing composite score metrics, and parsing model parameters.\n",
    "\n",
    "    Parameters:\n",
    "        mi_df (DataFrame): DataFrame containing MI evaluation results.\n",
    "        acc_df (DataFrame): DataFrame containing accuracy evaluation results.\n",
    "        target_models (list): List of model names to include in the analysis.\n",
    "        mi_metrics (list): List of MI metrics (strings) to retain in the analysis.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The merged and processed analysis table containing performance metrics,\n",
    "                   composite ISCORE, scaling factors, and parsed model parameters.\n",
    "    \"\"\"\n",
    "    # Filter evaluation tables based on the target models\n",
    "    mi_table = mi_df[mi_df['Model'].isin(target_models)].reset_index(drop=True)\n",
    "    acc_table = acc_df[acc_df['Model'].isin(target_models)].reset_index(drop=True)\n",
    "    \n",
    "    # Normalize MAPE and select required columns for accuracy table\n",
    "    if 'MAPEnorm' in acc_table.columns:\n",
    "        acc_table['MAPEnorm'] = acc_table['MAPEnorm'] / 100\n",
    "    acc_table = acc_table[['Model', 'MeanKldRes', 'RMSE']].copy()\n",
    "    acc_table.columns = ['Model', 'FQI', 'RMSE']\n",
    "    \n",
    "    # Process MI table: group by Model and Metrics, average values, then filter and pivot the table\n",
    "    mi_grouped = mi_table.groupby(['Model', 'Metrics']).mean().reset_index()\n",
    "    #mi_filtered = mi_grouped[mi_grouped['Metrics'].isin(mi_metrics)].reset_index(drop=True)\n",
    "    mi_pivot = pd.pivot(mi_grouped, index='Model', columns='Metrics', values='Values').reset_index()\n",
    "    mi_pivot = mi_pivot.fillna(0)\n",
    "    \n",
    "    # Merge MI and accuracy tables\n",
    "    merged_table = pd.merge(mi_pivot, acc_table, on='Model', how='inner').sort_values('Model').reset_index(drop=True)\n",
    "    \n",
    "    # Split the 'Model' string into structural parameters\n",
    "    split_cols = merged_table['Model'].str.split('_', expand=True)\n",
    "    if split_cols.shape[1] == 6:\n",
    "        split_cols.columns = ['Prefix', 'Type', 'Depth', 'LatDim', 'Comp', 'Source']\n",
    "        merged_table = pd.concat([merged_table, split_cols], axis=1)\n",
    "    elif split_cols.shape[1] == 4:\n",
    "        mask = split_cols[3].isna() | (split_cols[3] == 'None')\n",
    "        split_cols.loc[mask, 3] = split_cols.loc[mask, 2]\n",
    "        split_cols.loc[mask, 2] = 0\n",
    "        split_cols.columns = ['Prefix', 'Type', 'LatDim', 'Source']\n",
    "        merged_table = pd.concat([merged_table, split_cols], axis=1)\n",
    "    else:\n",
    "        print(\"Warning: Unexpected model naming format. Check the 'Model' column.\")\n",
    "    \n",
    "    # Compute composite information score (ISCORE)\n",
    "    merged_table['ISCORE'] = softplus(\n",
    "        merged_table['(i) $I(V; \\\\acute{Z} \\\\mid Z)$'] +\n",
    "        merged_table['(iii) $I(S;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$'] -\n",
    "        merged_table['(ii) $I(V;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$']\n",
    "    )\n",
    "    \n",
    "    # Compute scaling based on exponential of FQI and RMSE and then the scaled ISCORE\n",
    "    merged_table['Scaling'] = (np.exp(-merged_table['FQI']) + np.exp(-merged_table['RMSE'])) / 2\n",
    "    merged_table['ISCOREScal'] = merged_table['ISCORE'] * merged_table['Scaling']\n",
    "    \n",
    "    return merged_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Analysis Table and Perform ISCORE-Based Parameter Selection for Main Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration models and combine all model keys across configuration files\n",
    "config_directory = './Config/'\n",
    "TabLists = load_config_models(config_directory)\n",
    "AnalTabList = list(np.concatenate([tabs for key, tabs in TabLists.items()]))\n",
    "\n",
    "legend_map = {\n",
    "    'Depth': r'$\\zeta$',\n",
    "    'LatDim': r'$J$',\n",
    "    'Comp': r'$C$'\n",
    "}\n",
    "\n",
    "# Define ablation models to exclude from final analysis (if needed later)\n",
    "AblationList = [\n",
    "    'FC_ART_1_50_800_Mimic', 'FC_ART_1_50_800_VitalDB',\n",
    "    'SKZ_ART_1_50_800_Mimic', 'SKZ_ART_1_50_800_VitalDB',\n",
    "    'FC_II_1_50_800_Mimic', 'FC_II_1_50_800_VitalDB',\n",
    "    'SKZ_II_1_50_800_Mimic', 'SKZ_II_1_50_800_VitalDB'\n",
    "]\n",
    "\n",
    "# Define MI metrics to be used in the analysis\n",
    "AnalMetricList = [\n",
    "    '(i) $I(V; \\\\acute{Z} \\\\mid Z)$',\n",
    "    '(ii) $I(V;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$',\n",
    "    '(iii) $I(S;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$'\n",
    "]\n",
    "\n",
    "\n",
    "# Prepare the merged analysis table using the function\n",
    "AnalAccMItable = prepare_analysis_table(MItableSet, AcctableSet, AnalTabList, AnalMetricList)\n",
    "\n",
    "# Exclude ablation models from main analysis\n",
    "SenseAccMItable = AnalAccMItable[~AnalAccMItable['Model'].isin(AblationList)]\n",
    "\n",
    "\n",
    "# Parameter search: find optimal Depth, LatDim, Comp per Type based on ISCOREScal\n",
    "ParamSearch = pd.DataFrame()\n",
    "SelBestReport = pd.DataFrame()\n",
    "for metric in ['Depth', 'LatDim', 'Comp']:\n",
    "    ResTableGroup = SenseAccMItable.groupby(['Type', metric]).mean(numeric_only=True).reset_index()\n",
    "    Param = ResTableGroup.loc[ResTableGroup.groupby(\"Type\")[\"ISCOREScal\"].idxmax(), [\"Type\", metric, \"ISCOREScal\"]]\n",
    "    Param['Param'] = metric\n",
    "    Param = Param.rename(columns={metric : 'Value'})[['Type','Param', 'Value', 'ISCOREScal']]\n",
    "    ParamSearch = pd.concat([ParamSearch, Param], axis=0)\n",
    "\n",
    "    SelBest =  ResTableGroup[['Type', metric,'ISCOREScal']].copy()\n",
    "    SelBest['Hyperparameter'] = legend_map[metric]\n",
    "    SelBest = SelBest.rename(columns={metric:'Setting', 'ISCOREScal':'ISCORE'})\n",
    "    SelBest = SelBest[['Type','Hyperparameter','Setting', 'ISCORE']]\n",
    "    SelBestReport = pd.concat([SelBestReport, SelBest], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\caption{Group-wise sensitivity analysis results by type and hyperparameter.}\n",
      "\\label{tab:SelBestReport}\n",
      "\\begin{tabular}{cc|cc}\n",
      "\\toprule\n",
      "Hyperparameter & Setting & ABP & ECG \\\\\n",
      "\\midrule\n",
      "$C$ & 5s & 2.031 & 2.047 \\\\\n",
      "$C$ & 8s & \\textbf{2.036} & \\textbf{2.419} \\\\\n",
      "$J$ & 30 & 2.024 & 2.198 \\\\\n",
      "$J$ & 50 & \\textbf{2.054} & \\textbf{2.489} \\\\\n",
      "$\\zeta$ & 1 & \\textbf{2.159} & \\textbf{2.435} \\\\\n",
      "$\\zeta$ & 2 & 1.909 & 2.155 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Pivot to wide format: one row per (Hyperparameter, Setting), ISCORE values for ART and II as columns\n",
    "SelBestReport_wide = SelBestReport.pivot_table(\n",
    "    index=['Hyperparameter', 'Setting'],\n",
    "    columns='Type',\n",
    "    values='ISCORE'\n",
    ").reset_index()\n",
    "SelBestReport_wide = SelBestReport_wide.rename(columns={'ART': 'ABP', 'II': 'ECG'})\n",
    "SelBestReport_wide['Setting'] = SelBestReport_wide['Setting'].replace({'500': '5s', '800': '8s'})\n",
    "\n",
    "\n",
    "# Step 4: Bold the max ISCORE per Type within each Hyperparameter group\n",
    "for t in ['ABP', 'ECG']:\n",
    "    # Find the max value for each (Hyperparameter) group\n",
    "    max_mask = SelBestReport_wide.groupby('Hyperparameter')[t].transform('max') == SelBestReport_wide[t]\n",
    "    # Apply bold formatting for those max values\n",
    "    SelBestReport_wide[t] = SelBestReport_wide.apply(\n",
    "        lambda row: f\"\\\\textbf{{{row[t]:.3f}}}\" if max_mask.loc[row.name] else f\"{row[t]:.3f}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# Step 5: Convert to LaTeX with vertical line between Setting and metrics\n",
    "latex_code = r\"\"\"\\begin{table}[h]\n",
    "\\centering\n",
    "\\caption{Group-wise sensitivity analysis results by type and hyperparameter.}\n",
    "\\label{tab:SelBestReport}\n",
    "\"\"\" + SelBestReport_wide.to_latex(index=False, escape=False, column_format=\"cc|cc\") + r\"\"\"\\end{table}\"\"\"\n",
    "\n",
    "# Output LaTeX code\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Analysis Table (ISCORE-Based) for Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration models and combine all model keys across configuration files\n",
    "Bench_config_directory = './Benchmarks/Config/'\n",
    "BenchTabLists = load_config_models(Bench_config_directory)\n",
    "BenchAnalTabList = list(np.concatenate([tabs for key, tabs in BenchTabLists.items()]))\n",
    "\n",
    "# Prepare the merged analysis table using the function\n",
    "BenchAnalAccMItable = prepare_analysis_table(BenchMItableSet, BenchAcctableSet, BenchAnalTabList, AnalMetricList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert parameter search result into dictionary format:\n",
    "# e.g., ParamDict = {'ART': {'Depth': '1', 'LatDim': '50', 'Comp': '800'}, ...}\n",
    "ParamDict = { t: { row['Param']: row['Value']\n",
    "                    for _, row in ParamSearch[ParamSearch['Type'] == t].iterrows()\n",
    "                 }  for t in ParamSearch['Type'].unique() }\n",
    "\n",
    "\n",
    "BestModelList = pd.DataFrame()\n",
    "# Iterate over sources and types to find best models based on selected parameters\n",
    "for Source in ['Mimic','VitalDB']:\n",
    "    SelDataset = SenseAccMItable[SenseAccMItable['Source'] == Source]\n",
    "    for Type, Value in ParamDict.items():\n",
    "        SelModels = SenseAccMItable[(SenseAccMItable['Type'] == Type) & \n",
    "                        (SenseAccMItable['Depth'] == Value['Depth']) & \n",
    "                        (SenseAccMItable['LatDim'] == Value['LatDim']) & \n",
    "                        (SenseAccMItable['Comp'] == Value['Comp']) &\n",
    "                        (SenseAccMItable['Source'] == Source)]\n",
    "        BestModelList = pd.concat([BestModelList, SelModels]) \n",
    "\n",
    "# Build the comparison table including both ablation models and selected best models\n",
    "AblaAccMItable = AnalAccMItable[AnalAccMItable['Model'].isin( AblationList + BestModelList['Model'].tolist())]\n",
    "\n",
    "# Build the comparison table including both benchmark models and selected best models\n",
    "MainCompList = ['SKZFC_ART_1_30_800_Mimic',  'SKZFC_II_1_30_800_Mimic', 'SKZFC_ART_1_30_800_VitalDB', 'SKZFC_II_1_30_800_VitalDB']\n",
    "SelModelComp = AnalAccMItable[AnalAccMItable['Model'].isin( MainCompList)][BenchAnalAccMItable.columns]\n",
    "BenchAnalAccMItable = pd.concat([BenchAnalAccMItable, SelModelComp]).copy()\n",
    "\n",
    "\n",
    "# BenchAnalAccMItable[(BenchAnalAccMItable['Type'] =='II') & (BenchAnalAccMItable['Source'] =='Mimic')].sort_values('ISCOREScal')\n",
    "# 1. ART & VitalDB: SKZFC_ART_1_30_800_VitalDB, VDWave_ART_VitalDB, TCVAE_ART_30_VitalDB\n",
    "# 2. ART & Mimic: SKZFC_ART_1_30_800_Mimic, VDWave_ART_Mimic, TCVAE_ART_30_Mimic\n",
    "# 3. II & VitalDB: SKZFC_II_1_30_800_VitalDB, VDWave_II_VitalDB, TCVAE_II_30_VitalDB\n",
    "# 4. II & Mimic: SKZFC_II_1_30_800_Mimic, VDWave_II_Mimic, TCVAE_II_30_Mimic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Summary Tables to 'EvalResults/SummaryTables/' Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./EvalResults/SummaryTables/AnalAccMItable.csv\n",
      "Saved: ./EvalResults/SummaryTables/SenseAccMItable.csv\n",
      "Saved: ./EvalResults/SummaryTables/BenchAnalAccMItable.csv\n",
      "Saved: ./EvalResults/SummaryTables/AblaAccMItable.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the output directory\n",
    "output_dir = './EvalResults/SummaryTables/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define table name to DataFrame mapping\n",
    "tables_to_save = {\n",
    "    'AnalAccMItable.csv': AnalAccMItable,\n",
    "    'SenseAccMItable.csv': SenseAccMItable,\n",
    "    'BenchAnalAccMItable.csv': BenchAnalAccMItable,\n",
    "    'AblaAccMItable.csv': AblaAccMItable,\n",
    "}\n",
    "\n",
    "# Save each DataFrame to the specified directory\n",
    "for filename, df in tables_to_save.items():\n",
    "    save_path = os.path.join(output_dir, filename)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
