{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Module & Utility Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Utilities.EvaluationMain import *\n",
    "from Utilities.Utilities import ReadYaml, SerializeObjects, DeserializeObjects, LoadModelConfigs, LoadParams\n",
    "from Models.Caller64 import *\n",
    "from Utilities.Visualization import VisReconGivenZ_FCA, HeatMapFreqZ_FCA, VisReconGivenFC_ZA, VisReconExtractZ_FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Configurations and Evaluation Tables (Accuracy & MI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_tables(directory, acc_keyword, acc_pattern, mi_keyword, mi_pattern):\n",
    "    \"\"\"\n",
    "    Load and combine evaluation tables from a specified directory based on filtering keywords.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Path to the directory containing CSV table files.\n",
    "        acc_keyword (str): Keyword to identify accuracy tables.\n",
    "        acc_pattern (str): Additional substring that accuracy table filenames must contain.\n",
    "        mi_keyword (str): Keyword to identify MI (Mutual Information) tables.\n",
    "        mi_pattern (str): Additional substring that MI table filenames must contain.\n",
    "\n",
    "    Returns:\n",
    "        acc_df (DataFrame): A concatenated DataFrame of accuracy tables with an added 'RMSE' column.\n",
    "        mi_df (DataFrame): A concatenated DataFrame of MI tables.\n",
    "    \"\"\"\n",
    "    # List all files in the specified directory\n",
    "    table_list = os.listdir(directory)\n",
    "    \n",
    "    # Load and combine accuracy tables\n",
    "    acc_list = [tab for tab in table_list if acc_keyword in tab and acc_pattern in tab]\n",
    "    acc_df = pd.DataFrame()\n",
    "    for tab in acc_list:\n",
    "        file_path = os.path.join(directory, tab)\n",
    "        df = pd.read_csv(file_path)\n",
    "        acc_df = pd.concat([acc_df, df], axis=0)\n",
    "    # Compute RMSE if the 'MSEdenorm' column is available\n",
    "    if 'MSEdenorm' in acc_df.columns:\n",
    "        acc_df['RMSE'] = np.sqrt(acc_df['MSEdenorm'])\n",
    "    \n",
    "    # Load and combine MI tables\n",
    "    mi_list = [tab for tab in table_list if mi_keyword in tab and mi_pattern in tab]\n",
    "    mi_df = pd.DataFrame()\n",
    "    for tab in mi_list:\n",
    "        file_path = os.path.join(directory, tab)\n",
    "        df = pd.read_csv(file_path)\n",
    "        mi_df = pd.concat([mi_df, df], axis=0)\n",
    "    \n",
    "    return acc_df, mi_df\n",
    "\n",
    "\n",
    "def load_config_models(config_directory, include_keyword='Config', exclude_keyword='Eval', key='Models'):\n",
    "    \"\"\"\n",
    "    Load configuration files from the specified directory and extract model keys.\n",
    "\n",
    "    Parameters:\n",
    "        config_directory (str): Path to the directory containing YAML configuration files.\n",
    "        include_keyword (str): Only consider files that include this keyword.\n",
    "        exclude_keyword (str): Exclude files that contain this keyword.\n",
    "        key (str): The key in the YAML file from which to extract model definitions.\n",
    "\n",
    "    Returns:\n",
    "        model_dict (dict): A dictionary mapping configuration file names (without extension)\n",
    "                           to a list of model keys.\n",
    "    \"\"\"\n",
    "    config_files = [f for f in os.listdir(config_directory)\n",
    "                    if include_keyword in f and exclude_keyword not in f]\n",
    "    model_dict = {}\n",
    "    for config in config_files:\n",
    "        full_path = os.path.join(config_directory, config)\n",
    "        config_data = ReadYaml(full_path)\n",
    "        model_dict[config.split('.')[0]] = list(config_data.get(key, {}).keys())\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main evaluation tables\n",
    "eval_directory = './EvalResults/Tables/'\n",
    "AcctableSet, MItableSet = load_evaluation_tables(\n",
    "    eval_directory,\n",
    "    acc_keyword='Acc',\n",
    "    acc_pattern='Nj1_FC',\n",
    "    mi_keyword='MI',\n",
    "    mi_pattern='Nj1_FC')\n",
    "\n",
    "\n",
    "# Benchmark evaluation tables\n",
    "bench_directory = './Benchmarks/EvalResults/Tables/'\n",
    "BenchAcctableSet, BenchMItableSet = load_evaluation_tables(\n",
    "    bench_directory,\n",
    "    acc_keyword='Acc',\n",
    "    acc_pattern='NjAll',\n",
    "    mi_keyword='MI',\n",
    "    mi_pattern='NjAll')\n",
    "\n",
    "# Define a mapping for metrics to be unified\n",
    "metrics_map = {\n",
    "    '(i) $I(V;\\\\acute{\\\\Theta} \\\\mid X)$': '(ii) $I(V;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$',\n",
    "    '(ii) $I(S;\\\\acute{\\\\Theta} \\\\mid X)$': '(iii) $I(S;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$'}\n",
    "\n",
    "# Create a new column ('UnifiedMetric') while preserving the original 'Metrics'\n",
    "BenchMItableSet['Metrics'] = BenchMItableSet['Metrics'].replace(metrics_map)\n",
    "\n",
    "# Extract unique metric types\n",
    "MetricTypes = np.unique(MItableSet['MetricType']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Construct Analysis Table and Perform ISCORE-Based Parameter Selection for Main Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_models(config_directory, include_keyword='Config', exclude_keyword='Eval', key='Models'):\n",
    "    \"\"\"\n",
    "    Load configuration files from the specified directory and extract model keys.\n",
    "\n",
    "    Parameters:\n",
    "        config_directory (str): Path to the directory containing YAML configuration files.\n",
    "        include_keyword (str): Only consider files that include this keyword.\n",
    "        exclude_keyword (str): Exclude files that contain this keyword.\n",
    "        key (str): The key in the YAML file from which to extract model definitions.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping configuration file names (without extension)\n",
    "              to a list of model keys.\n",
    "    \"\"\"\n",
    "    config_files = [f for f in os.listdir(config_directory)\n",
    "                    if include_keyword in f and exclude_keyword not in f]\n",
    "    model_dict = {}\n",
    "    for config in config_files:\n",
    "        full_path = os.path.join(config_directory, config)\n",
    "        config_data = ReadYaml(full_path)\n",
    "        model_dict[config.split('.')[0]] = list(config_data.get(key, {}).keys())\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "\n",
    "def prepare_analysis_table(mi_df, acc_df, target_models):\n",
    "    \"\"\"\n",
    "    Prepare the analysis table by merging MI and accuracy data, filtering by target models and metrics,\n",
    "    computing composite score metrics, and parsing model parameters.\n",
    "    Parameters:\n",
    "        mi_df (DataFrame): DataFrame containing MI evaluation results.\n",
    "        acc_df (DataFrame): DataFrame containing accuracy evaluation results.\n",
    "        target_models (list): List of model names to include in the analysis.\n",
    "    Returns:\n",
    "        DataFrame: The merged and processed analysis table containing performance metrics,\n",
    "                   composite ISCORE, scaling factors, and parsed model parameters.\n",
    "    \"\"\"\n",
    "    # Filter evaluation tables based on the target models\n",
    "    mi_table = mi_df[mi_df['Model'].isin(target_models)].reset_index(drop=True)\n",
    "    acc_table = acc_df[acc_df['Model'].isin(target_models)].reset_index(drop=True)\n",
    "    \n",
    "    # Normalize MAPE and select required columns for accuracy table\n",
    "    if 'MAPEnorm' in acc_table.columns:\n",
    "        acc_table['MAPEnorm'] = acc_table['MAPEnorm'] / 100\n",
    "    acc_table = acc_table[['Model', 'MeanKldRes', 'RMSE', 'R2denorm']].copy()\n",
    "    acc_table.columns = ['Model', 'FQI', 'RMSE', 'R2denorm']\n",
    "    \n",
    "    # Process MI table: group by Model and Metrics, average values, then filter and pivot the table\n",
    "    mi_grouped = mi_table.groupby(['Model', 'Metrics']).mean(numeric_only=True).reset_index()\n",
    "    mi_pivot = pd.pivot(mi_grouped, index='Model', columns='Metrics', values='Values').reset_index()\n",
    "    \n",
    "    # Merge MI and accuracy tables\n",
    "    merged_table = pd.merge(mi_pivot, acc_table, on='Model', how='inner').sort_values('Model').reset_index(drop=True)\n",
    "    \n",
    "    # Split the 'Model' string into structural parameters\n",
    "    split_cols = merged_table['Model'].str.split('_', expand=True)\n",
    "    if split_cols.shape[1] == 6:\n",
    "        split_cols.columns = ['Prefix', 'Type', 'Depth', 'LatDim', 'Comp', 'Source']\n",
    "        merged_table = pd.concat([merged_table, split_cols], axis=1)\n",
    "    elif split_cols.shape[1] == 4:\n",
    "        mask = split_cols[3].isna() | (split_cols[3] == 'None')\n",
    "        split_cols.loc[mask, 3] = split_cols.loc[mask, 2]\n",
    "        split_cols.loc[mask, 2] = 0\n",
    "        split_cols.columns = ['Prefix', 'Type', 'LatDim', 'Source']\n",
    "        merged_table = pd.concat([merged_table, split_cols], axis=1)\n",
    "    else:\n",
    "        print(\"Warning: Unexpected model naming format. Check the 'Model' column.\")\n",
    "    \n",
    "    # Compute composite integrated score (ISCORE)\n",
    "    ms_metric_col = '(i) $I(V; \\\\acute{Z} \\\\mid Z)$'\n",
    "    \n",
    "    # Common metrics for all models\n",
    "    merged_table['MP'] = 1-np.sqrt(1 - np.exp(-2 * merged_table['(ii) $I(V;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$']))\n",
    "    merged_table['AC'] = np.sqrt(1 - np.exp(-2 * merged_table['(iii) $I(S;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$']))\n",
    "    merged_table['SS'] = 1-np.sqrt(1 - np.exp(-2 * merged_table['FQI']))\n",
    "    merged_table['RA'] = merged_table['R2denorm']\n",
    "    merged_table['NMSE'] = 1 - merged_table['RA']\n",
    "\n",
    "    \n",
    "    # Handle MS metric if column exists\n",
    "    if ms_metric_col in merged_table.columns:\n",
    "        # Calculate MS only for rows with non-null values\n",
    "        mask_has_ms = merged_table[ms_metric_col].notna()\n",
    "        merged_table.loc[mask_has_ms, 'MS'] = np.sqrt(1 - np.exp(-2 * merged_table.loc[mask_has_ms, ms_metric_col]))\n",
    "        \n",
    "        # Calculate ISCORE with MS for models that have it (AM > GM > HM order)\n",
    "        merged_table.loc[mask_has_ms, 'ISCOREam'] = mean(merged_table.loc[mask_has_ms, ['MS', 'MP', 'AC', 'SS', 'RA']], kind=\"am\", axis=1)\n",
    "        merged_table.loc[mask_has_ms, 'ISCOREgm'] = mean(merged_table.loc[mask_has_ms, ['MS', 'MP', 'AC', 'SS', 'RA']], kind=\"gm\", axis=1)\n",
    "        merged_table.loc[mask_has_ms, 'ISCOREhm'] = mean(merged_table.loc[mask_has_ms, ['MS', 'MP', 'AC', 'SS', 'RA']], kind=\"hm\", axis=1)\n",
    "        \n",
    "        # Calculate ISCORE without MS for models that don't have it (AM > GM > HM order)\n",
    "        mask_no_ms = merged_table[ms_metric_col].isna()\n",
    "        merged_table.loc[mask_no_ms, 'ISCOREam'] = mean(merged_table.loc[mask_no_ms, ['MP', 'AC', 'SS', 'RA']], kind=\"am\", axis=1)\n",
    "        merged_table.loc[mask_no_ms, 'ISCOREgm'] = mean(merged_table.loc[mask_no_ms, ['MP', 'AC', 'SS', 'RA']], kind=\"gm\", axis=1)\n",
    "        merged_table.loc[mask_no_ms, 'ISCOREhm'] = mean(merged_table.loc[mask_no_ms, ['MP', 'AC', 'SS', 'RA']], kind=\"hm\", axis=1)\n",
    "    else:\n",
    "        # Calculate ISCORE without MS for all models (AM > GM > HM order)\n",
    "        merged_table['ISCOREam'] = mean(merged_table[['MP', 'AC', 'SS', 'RA']], kind=\"am\", axis=1)\n",
    "        merged_table['ISCOREgm'] = mean(merged_table[['MP', 'AC', 'SS', 'RA']], kind=\"gm\", axis=1)\n",
    "        merged_table['ISCOREhm'] = mean(merged_table[['MP', 'AC', 'SS', 'RA']], kind=\"hm\", axis=1)\n",
    "        \n",
    "        \n",
    "    return merged_table\n",
    "    \n",
    "def amean(x, weights=None, axis=None, eps=None):\n",
    "    \"\"\"\n",
    "    Arithmetic mean (simple or weighted).\n",
    "    If weights is None: AM = mean(x, axis).\n",
    "    If weights is provided: AM = sum(w * x) / sum(w) along the given axis.\n",
    "    Note: `eps` is unused here; included only for API symmetry with gmean/hmean.\n",
    "    \"\"\"\n",
    "    if isinstance(x, (pd.DataFrame, pd.Series)):\n",
    "        x = x.values\n",
    "\n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    if weights is None:\n",
    "        return np.mean(x, axis=axis)\n",
    "\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    # np.average supports weights for both axis=None and axis=int\n",
    "    return np.average(x, weights=w, axis=axis)\n",
    "    \n",
    "def gmean(x, weights=None, axis=None, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Geometric mean for non-negative values (typical use: scores in [0,1]).\n",
    "    Stabilized with eps: GM = exp( average(log(x + eps)) ).\n",
    "    If any entry is exactly 0, result trends toward 0 (controlled by eps).\n",
    "    \"\"\"\n",
    "    if isinstance(x, (pd.DataFrame, pd.Series)):\n",
    "        x = x.values\n",
    "    \n",
    "    x = np.asarray(x, dtype=float)\n",
    "    logs = np.log(x + eps)\n",
    "    if weights is None:\n",
    "        return np.exp(np.mean(logs, axis=axis))\n",
    "    return np.exp(np.average(logs, weights=np.asarray(weights, dtype=float), axis=axis))\n",
    "\n",
    "def hmean(x, weights=None, axis=None, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Harmonic mean for strictly positive values.\n",
    "    Stabilized with eps to avoid division by zero: H = sum(w) / sum(w / (x + eps))\n",
    "    If any entry is near 0, HM drops strongly (designed behavior).\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(x, (pd.DataFrame, pd.Series)):\n",
    "        x = x.values\n",
    "    \n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if weights is None:\n",
    "        denom = np.sum(1.0 / (x + eps), axis=axis)\n",
    "        count = x.size if axis is None else x.shape[axis]\n",
    "        return count / denom\n",
    "    w = np.asarray(weights, dtype=float)\n",
    "    return np.sum(w, axis=axis) / np.sum(w / (x + eps), axis=axis)\n",
    "\n",
    "def mean(x, kind=\"gm\", weights=None, axis=None, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Unified interface:\n",
    "      kind ∈ {\"am\",\"gm\",\"hm\"} for arithmetic / geometric / harmonic.\n",
    "    Supports pandas DataFrame/Series input.\n",
    "    \"\"\"\n",
    "    kind = kind.lower()\n",
    "    if kind == \"am\":\n",
    "        return amean(x, weights=weights, axis=axis)\n",
    "    if kind == \"gm\":\n",
    "        return gmean(x, weights=weights, axis=axis, eps=eps)\n",
    "    if kind == \"hm\":\n",
    "        return hmean(x, weights=weights, axis=axis, eps=eps)\n",
    "    raise ValueError(\"kind must be one of {'am','gm','hm'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Analysis Table and Perform ISCORE-Based Parameter Selection for Main Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration models and combine all model keys across configuration files\n",
    "config_directory = './Config/'\n",
    "TabLists = load_config_models(config_directory)\n",
    "AnalTabList = list(np.concatenate([tabs for key, tabs in TabLists.items()]))\n",
    "\n",
    "legend_map = {\n",
    "    'Depth': r'$\\zeta$',\n",
    "    'LatDim': r'$J$',\n",
    "    'Comp': r'$C$'\n",
    "}\n",
    "\n",
    "# Define ablation models to exclude from final analysis (if needed later)\n",
    "AblationList = [\n",
    "    'FC_ART_1_30_800_Mimic', 'SKZ_ART_1_30_800_Mimic',\n",
    "    'FC_ART_1_50_800_VitalDB', 'SKZ_ART_1_50_800_VitalDB',\n",
    "    'FC_II_1_50_800_Mimic', 'SKZ_II_1_50_800_Mimic',\n",
    "    'FC_II_1_50_800_VitalDB', 'SKZ_II_1_50_800_VitalDB'\n",
    "]\n",
    "\n",
    "MainList = [\n",
    "    'SKZFC_ART_1_30_800_Mimic',\n",
    "    'SKZFC_ART_1_30_800_VitalDB',\n",
    "    'SKZFC_II_1_30_800_Mimic',\n",
    "    'SKZFC_II_1_30_800_VitalDB'\n",
    "]\n",
    "\n",
    "ExclusionList = ['FC_ART_1_50_800_Mimic',  'SKZ_ART_1_50_800_Mimic']\n",
    "\n",
    "# Define MI metrics to be used in the analysis\n",
    "AnalMetricList = [\n",
    "    '(i) $I(V; \\\\acute{Z} \\\\mid Z)$',\n",
    "    '(ii) $I(V;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$',\n",
    "    '(iii) $I(S;\\\\acute{\\\\Theta} \\\\mid \\\\acute{Z})$'\n",
    "]\n",
    "\n",
    "\n",
    "# Prepare the merged analysis table using the function\n",
    "AnalAccMItableDic = {}\n",
    "SubAcctableSet = AcctableSet.copy()\n",
    "for mtype in MetricTypes:\n",
    "    SubMItableSet = MItableSet[MItableSet['MetricType'] == mtype].reset_index(drop=True)\n",
    "    KldCols = [col for col in AcctableSet.columns if 'MeanKld' in col]\n",
    "    SelKldCols = [type for type in KldCols if mtype in type]\n",
    "    SubAcctableSet['MeanKldRes'] = AcctableSet[SelKldCols]\n",
    "    AnalAccMItableDic[mtype] = prepare_analysis_table(SubMItableSet, SubAcctableSet, AnalTabList)\n",
    "\n",
    "AnalAccMItableMerged = pd.concat(AnalAccMItableDic, axis=0)\n",
    "AnalAccMItableMerged = AnalAccMItableMerged.reset_index(level=0).rename(columns={'level_0': 'MetricType'})\n",
    "SenseAccMItable = AnalAccMItableMerged[~AnalAccMItableMerged['Model'].isin(AblationList+ExclusionList+MainList)].copy()\n",
    "SenseAccMItable = SenseAccMItable[['MetricType','Model', 'Source', 'Type', 'Depth', 'LatDim', 'Comp', 'MS', 'MP', 'AC', 'SS', 'RA', 'ISCOREam', 'ISCOREgm', 'ISCOREhm']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Analysis Table (ISCORE-Based) for Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration models and combine all model keys across configuration files\n",
    "Bench_config_directory = './Benchmarks/Config/'\n",
    "BenchTabLists = load_config_models(Bench_config_directory)\n",
    "BenchAnalTabList = list(np.concatenate([tabs for key, tabs in BenchTabLists.items()]))\n",
    "\n",
    "# Prepare the merged analysis table using the function\n",
    "BenchAnalAccMItableDic = {}\n",
    "SubBenchAcctableSet = BenchAcctableSet.copy()\n",
    "for mtype in MetricTypes:\n",
    "    SubBenchMItableSet = BenchMItableSet[BenchMItableSet['MetricType'] == mtype].reset_index(drop=True)\n",
    "    KldCols = [col for col in BenchAcctableSet.columns if 'MeanKld' in col]\n",
    "    SelKldCols = [type for type in KldCols if mtype in type]\n",
    "    SubBenchAcctableSet['MeanKldRes'] = BenchAcctableSet[SelKldCols]\n",
    "    BenchAnalAccMItableDic[mtype] = prepare_analysis_table(SubBenchMItableSet, SubBenchAcctableSet,  BenchAnalTabList)\n",
    "\n",
    "BenchAnalAccMItableMerged = pd.concat(BenchAnalAccMItableDic)\n",
    "BenchAnalAccMItableMerged = BenchAnalAccMItableMerged.reset_index(level=0).rename(columns={'level_0': 'MetricType'})\n",
    "RAReferences = BenchAnalAccMItableMerged[['Type','Source', 'RA']].groupby(['Type','Source']).min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Per-Group Best Models and Compute Cross-Method Aggregated Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Per-method best selection for each metric\\nBestSerchDic = {}\\n\\nfor key, AnalAccMItable in AnalAccMItableDic.items():\\n    SubSenseAccMItable = AnalAccMItable[~AnalAccMItable[\\'Model\\'].isin(AblationList+ExclusionList)].copy()\\n    \\n    PerformanceTab = SubSenseAccMItable[[\\'Source\\', \\'Type\\', \\'Model\\',\\'ISCOREam\\', \\'ISCOREgm\\', \\'ISCOREhm\\']].copy()\\n    PerformanceTablong = pd.DataFrame()\\n    for iscore_col in [\\'ISCOREam\\', \\'ISCOREgm\\', \\'ISCOREhm\\']:\\n        # pick max row per (Type, Source)\\n        SelPerformanceTab = PerformanceTab.loc[PerformanceTab.groupby([\"Type\", \\'Source\\'])[iscore_col].idxmax(), [\"Type\", \\'Source\\', \\'Model\\', iscore_col]]\\n        SelPerformanceTab = SelPerformanceTab.rename(columns={iscore_col:\\'Value\\'}) \\n        SelPerformanceTab[\\'MetricType\\'] = iscore_col[6:]  # \\'am\\',\\'gm\\',\\'hm\\'\\n        PerformanceTablong = pd.concat([PerformanceTablong, SelPerformanceTab], axis=0)\\n    BestSerchDic[key] = PerformanceTablong.reset_index(drop=True)[[\\'Source\\', \\'Type\\', \\'MetricType\\', \\'Model\\',\\'Value\\']]\\n    \\n\\n# Aggregate scores across all methods\\nMetricType = [\\'ISCOREam\\', \\'ISCOREgm\\', \\'ISCOREhm\\']\\nAnalAccMItableBinding = pd.DataFrame()\\nSenseAccMItable = pd.DataFrame()\\nfor key, AnalAccMItable in AnalAccMItableDic.items():\\n    SubSenseAccMItable = AnalAccMItable[~AnalAccMItable[\\'Model\\'].isin(AblationList+ExclusionList+MainList)].copy()\\n    SubSenseAccMItable[\\'MetricType\\'] = key\\n    AnalAccMItableBinding = pd.concat([AnalAccMItableBinding, SubSenseAccMItable])\\n    SenseAccMItable = pd.concat([SenseAccMItable, SubSenseAccMItable])\\nAnalAccMItableBinding = AnalAccMItableBinding[[\\'Source\\', \\'Type\\', \\'Model\\'] + MetricType].reset_index(drop=True)\\nSenseAccMItable = SenseAccMItable[[\\'MetricType\\',\\'Model\\', \\'Source\\', \\'Type\\', \\'Depth\\', \\'LatDim\\', \\'Comp\\', \\'MS\\', \\'MP\\', \\'AC\\', \\'SS\\', \\'RA\\', \\'ISCOREam\\', \\'ISCOREgm\\', \\'ISCOREhm\\']]\\n\\n# sum per (Source, Type, Model)\\nAnalAccMItableRowSum = AnalAccMItableBinding.groupby([\\'Source\\',\\'Type\\',\\'Model\\']).sum(numeric_only=True)\\n\\n# mean over all methods × metrics\\nAnalAccMItableAvg = pd.DataFrame(AnalAccMItableRowSum.sum(1) / (len(AnalAccMItableDic.keys()) * len(MetricType)), columns=[\\'agg_metric\\']).reset_index()\\n\\n# pick max agg_metric per (Type, Source)\\nBestModels = AnalAccMItableAvg.loc[AnalAccMItableAvg.groupby([\"Type\", \\'Source\\'])[\\'agg_metric\\'].idxmax(), [\"Type\", \\'Source\\', \\'Model\\',\\'agg_metric\\']]\\nBestModels = pd.merge(BestModels[[\\'Model\\', \\'agg_metric\\']], AnalAccMItableMerged, on=\\'Model\\').reset_index(drop=True)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Per-method best selection for each metric\n",
    "BestSerchDic = {}\n",
    "\n",
    "for key, AnalAccMItable in AnalAccMItableDic.items():\n",
    "    SubSenseAccMItable = AnalAccMItable[~AnalAccMItable['Model'].isin(AblationList+ExclusionList)].copy()\n",
    "    \n",
    "    PerformanceTab = SubSenseAccMItable[['Source', 'Type', 'Model','ISCOREam', 'ISCOREgm', 'ISCOREhm']].copy()\n",
    "    PerformanceTablong = pd.DataFrame()\n",
    "    for iscore_col in ['ISCOREam', 'ISCOREgm', 'ISCOREhm']:\n",
    "        # pick max row per (Type, Source)\n",
    "        SelPerformanceTab = PerformanceTab.loc[PerformanceTab.groupby([\"Type\", 'Source'])[iscore_col].idxmax(), [\"Type\", 'Source', 'Model', iscore_col]]\n",
    "        SelPerformanceTab = SelPerformanceTab.rename(columns={iscore_col:'Value'}) \n",
    "        SelPerformanceTab['MetricType'] = iscore_col[6:]  # 'am','gm','hm'\n",
    "        PerformanceTablong = pd.concat([PerformanceTablong, SelPerformanceTab], axis=0)\n",
    "    BestSerchDic[key] = PerformanceTablong.reset_index(drop=True)[['Source', 'Type', 'MetricType', 'Model','Value']]\n",
    "    \n",
    "\n",
    "# Aggregate scores across all methods\n",
    "MetricType = ['ISCOREam', 'ISCOREgm', 'ISCOREhm']\n",
    "AnalAccMItableBinding = pd.DataFrame()\n",
    "SenseAccMItable = pd.DataFrame()\n",
    "for key, AnalAccMItable in AnalAccMItableDic.items():\n",
    "    SubSenseAccMItable = AnalAccMItable[~AnalAccMItable['Model'].isin(AblationList+ExclusionList+MainList)].copy()\n",
    "    SubSenseAccMItable['MetricType'] = key\n",
    "    AnalAccMItableBinding = pd.concat([AnalAccMItableBinding, SubSenseAccMItable])\n",
    "    SenseAccMItable = pd.concat([SenseAccMItable, SubSenseAccMItable])\n",
    "AnalAccMItableBinding = AnalAccMItableBinding[['Source', 'Type', 'Model'] + MetricType].reset_index(drop=True)\n",
    "SenseAccMItable = SenseAccMItable[['MetricType','Model', 'Source', 'Type', 'Depth', 'LatDim', 'Comp', 'MS', 'MP', 'AC', 'SS', 'RA', 'ISCOREam', 'ISCOREgm', 'ISCOREhm']]\n",
    "\n",
    "# sum per (Source, Type, Model)\n",
    "AnalAccMItableRowSum = AnalAccMItableBinding.groupby(['Source','Type','Model']).sum(numeric_only=True)\n",
    "\n",
    "# mean over all methods × metrics\n",
    "AnalAccMItableAvg = pd.DataFrame(AnalAccMItableRowSum.sum(1) / (len(AnalAccMItableDic.keys()) * len(MetricType)), columns=['agg_metric']).reset_index()\n",
    "\n",
    "# pick max agg_metric per (Type, Source)\n",
    "BestModels = AnalAccMItableAvg.loc[AnalAccMItableAvg.groupby([\"Type\", 'Source'])['agg_metric'].idxmax(), [\"Type\", 'Source', 'Model','agg_metric']]\n",
    "BestModels = pd.merge(BestModels[['Model', 'agg_metric']], AnalAccMItableMerged, on='Model').reset_index(drop=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated Model Performance & Hyperparameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =============================================\n",
    "# 1.Aggregate performance across all methods\n",
    "# =============================================\n",
    "\n",
    "# Combine all per-method DataFrames into one table\n",
    "all_methods_df = pd.concat(AnalAccMItableDic.values(), ignore_index=True)\n",
    "\n",
    "# Exclude ablation models and other models in ExclusionList\n",
    "non_ablation_df = all_methods_df[~all_methods_df['Model'].isin(AblationList + ExclusionList)].copy()\n",
    "\n",
    "# Collect models with RA below the reference threshold into RemoveList\n",
    "RemoveList = []\n",
    "for idx, row in non_ablation_df.iterrows():\n",
    "    target_mask = (RAReferences['Type'] == row['Type']) & (RAReferences['Source'] == row['Source'])\n",
    "    if row['RA'] < RAReferences[target_mask]['RA'].values[0]:\n",
    "        RemoveList.append(row['Model'])\n",
    "RemoveList = list(set(RemoveList))  # Deduplicate\n",
    "\n",
    "# Compute mean ISCOREgm for each (Source, Type, Model)\n",
    "group_cols = ['Source', 'Type', 'Model']\n",
    "MetricType = ['ISCOREgm']\n",
    "aggregated_scores = non_ablation_df.groupby(group_cols)[MetricType].mean().reset_index()\n",
    "\n",
    "# Exclude models in RemoveList\n",
    "aggregated_scores_best = aggregated_scores.copy()\n",
    "aggregated_scores_best = aggregated_scores_best[~aggregated_scores_best['Model'].isin(RemoveList)]\n",
    "\n",
    "# Number of methods and metrics (for reference)\n",
    "n_methods = len(AnalAccMItableDic)\n",
    "n_metrics = len(MetricType)\n",
    "\n",
    "# 2. Select Best Model per (Type, Source)\n",
    "# =========================\n",
    "# Pick the row index with the maximum ISCOREgm for each (Type, Source)\n",
    "best_model_indices = aggregated_scores_best.groupby(['Type', 'Source'])['ISCOREgm'].idxmax()\n",
    "BestOverallModels = aggregated_scores_best.loc[best_model_indices, ['Type', 'Source', 'Model', 'ISCOREgm']].reset_index(drop=True)\n",
    "BestModels = pd.merge(BestOverallModels[['Model']], AnalAccMItableMerged, on='Model').reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# 3. Unified Sensitivity Analysis Function\n",
    "# =========================\n",
    "def calculate_sensitivity(df, hp, score_col, group_by=('Type','Source'),\n",
    "                          latex_labels=True, legend_map=None,\n",
    "                          n_methods=None, n_metrics=None):\n",
    "    \"\"\"\n",
    "    Compute sensitivity statistics for a given hyperparameter (hp) with LaTeX-ready labels.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Table containing model scores and hyperparameters.\n",
    "        hp (str): Hyperparameter name (e.g., 'Depth', 'LatDim', 'Comp').\n",
    "        score_col (str): Score column to evaluate (e.g., 'ISCOREgm').\n",
    "        group_by (tuple): Group keys excluding hp (default ('Type','Source')).\n",
    "        latex_labels (bool): If True, render Hyperparameter label in LaTeX-friendly form.\n",
    "        legend_map (dict): Optional mapping {hp_key: latex_label_string}; if present, use value directly.\n",
    "        n_methods, n_metrics (int): Optional counts to annotate effective data points.\n",
    "\n",
    "    Returns:\n",
    "        summary_stats (pd.DataFrame), detail_stats (pd.DataFrame)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # If hp column is missing, return (None, None)\n",
    "    if hp not in df.columns:\n",
    "        return None, None\n",
    "\n",
    "    legend_map = legend_map or {}\n",
    "\n",
    "    # (1) Detailed statistics per (group_by..., hp)\n",
    "    group_cols_detail = list(group_by) + [hp]\n",
    "    detail_stats = (\n",
    "        df.groupby(group_cols_detail)[score_col]\n",
    "          .agg(mean='mean', std='std', max='max', min='min', n='size')\n",
    "          .reset_index()\n",
    "          .rename(columns={hp: 'Setting'})\n",
    "    )\n",
    "\n",
    "    # (2) Build display label for the Hyperparameter (legend_map has highest priority)\n",
    "    if latex_labels:\n",
    "        if hp in legend_map and isinstance(legend_map[hp], str) and legend_map[hp].strip():\n",
    "            hp_label = legend_map[hp]  # Use mapped value as-is (e.g., r'$\\zeta$')\n",
    "        else:\n",
    "            # Fallback labels if no mapping provided\n",
    "            fallback = {'Depth': r'$\\zeta$', 'LatDim': r'$J$', 'Comp': r'$C$'}\n",
    "            hp_label = fallback.get(hp, hp)\n",
    "            # If fallback is not already in math mode, wrap with \\mathrm{...}\n",
    "            if isinstance(hp_label, str) and not (hp_label.startswith('$') and hp_label.endswith('$')):\n",
    "                hp_label = rf'$\\mathrm{{{hp_label}}}$'\n",
    "    else:\n",
    "        # When LaTeX labels are disabled, use raw mapping or hp name\n",
    "        hp_label = legend_map.get(hp, hp)\n",
    "\n",
    "    detail_stats['Hyperparameter'] = hp_label\n",
    "\n",
    "    # (3) Optional: annotate effective data points based on methods × metrics aggregation\n",
    "    if (n_methods is not None) and (n_metrics is not None):\n",
    "        total_points = n_methods * n_metrics\n",
    "        detail_stats['n_effective'] = detail_stats['n'] * total_points\n",
    "        detail_stats['note'] = f'Each model aggregated from {n_methods}x{n_metrics}={total_points} data points'\n",
    "\n",
    "    # (4) Summary statistics per (group_by..., Hyperparameter)\n",
    "    group_cols_summary = list(group_by) + ['Hyperparameter']\n",
    "    summary_stats = (\n",
    "        detail_stats.groupby(group_cols_summary)\n",
    "        .agg(\n",
    "            SensRange=('mean', lambda x: x.max() - x.min()),\n",
    "            SensStd=('mean', 'std'),\n",
    "            N_Settings=('Setting', 'nunique'),\n",
    "            Total_Models=('n', 'sum')\n",
    "        )\n",
    "        .reset_index()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    # If effective counts exist, aggregate them at the same granularity\n",
    "    if 'n_effective' in detail_stats.columns:\n",
    "        tmp = (detail_stats.groupby(group_cols_summary)['n_effective']\n",
    "               .sum().reset_index(name='Total_Effective_DataPoints'))\n",
    "        summary_stats = summary_stats.merge(tmp, on=group_cols_summary, how='left')\n",
    "\n",
    "    # (5) Column ordering for clarity\n",
    "    summary_cols = list(group_by) + ['Hyperparameter', 'SensStd', 'SensRange', 'N_Settings', 'Total_Models']\n",
    "    if 'Total_Effective_DataPoints' in summary_stats.columns:\n",
    "        summary_cols.append('Total_Effective_DataPoints')\n",
    "\n",
    "    detail_cols = list(group_by) + ['Hyperparameter', 'Setting', 'mean', 'std', 'max', 'min', 'n']\n",
    "    if 'n_effective' in detail_stats.columns:\n",
    "        detail_cols += ['n_effective', 'note']\n",
    "\n",
    "    return summary_stats[summary_cols], detail_stats[detail_cols]\n",
    "\n",
    "# =========================\n",
    "# 4. Aggregated Sensitivity Analysis\n",
    "# =========================\n",
    "ParamCols = ['Depth', 'LatDim', 'Comp']\n",
    "\n",
    "# Use the first method's DataFrame as a template to recover hyperparameter values\n",
    "template_df = next(iter(AnalAccMItableDic.values()))\n",
    "merge_cols = ['Source', 'Type', 'Model'] + ParamCols\n",
    "template_df = template_df[~template_df['Model'].isin(AblationList+ExclusionList)]\n",
    "template_subset = template_df.drop_duplicates(subset=merge_cols)[merge_cols]\n",
    "\n",
    "# Merge aggregated scores with hyperparameter columns\n",
    "df_with_agg = pd.merge(aggregated_scores, template_subset, on=['Source', 'Type', 'Model'], how='left')\n",
    "\n",
    "# Run sensitivity analysis for each hyperparameter on the aggregated metric\n",
    "summary_list, detail_list = [], []\n",
    "for hp in ParamCols:\n",
    "    summary, detail = calculate_sensitivity(df_with_agg, hp, 'ISCOREgm', n_methods=n_methods, n_metrics=n_metrics)\n",
    "    if summary is not None:\n",
    "        summary_list.append(summary)\n",
    "        detail_list.append(detail)\n",
    "\n",
    "AggSensitivityDic = pd.concat(summary_list, ignore_index=True) if summary_list else pd.DataFrame()\n",
    "AggSensitivityDetailDic = pd.concat(detail_list, ignore_index=True) if detail_list else pd.DataFrame()\n",
    "\n",
    "# =========================\n",
    "# 5. Individual Method Sensitivity Analysis (Optional)\n",
    "# =========================\n",
    "MetricTypesKey = ['fft', 'matching_pursuit', 'welch_evo']\n",
    "IScoreKey = ['ISCOREam', 'ISCOREgm', 'ISCOREhm']\n",
    "\n",
    "SensitivityDic, SensitivityDetailDic = {}, {}\n",
    "\n",
    "for mtype in MetricTypesKey:\n",
    "    # Exclude ablation models per method\n",
    "    df = AnalAccMItableDic[mtype][~AnalAccMItableDic[mtype]['Model'].isin(AblationList+ExclusionList)]\n",
    "\n",
    "    SensitivityDic[mtype] = {}\n",
    "    SensitivityDetailDic[mtype] = {}\n",
    "\n",
    "    for iscore_col in IScoreKey:\n",
    "        summary_list, detail_list = [], []\n",
    "        for hp in ParamCols:\n",
    "            summary, detail = calculate_sensitivity(df, hp, iscore_col)\n",
    "            if summary is not None:\n",
    "                summary_list.append(summary)\n",
    "                detail_list.append(detail)\n",
    "\n",
    "        SensitivityDic[mtype][iscore_col] = pd.concat(summary_list, ignore_index=True) if summary_list else pd.DataFrame()\n",
    "        SensitivityDetailDic[mtype][iscore_col] = pd.concat(detail_list, ignore_index=True) if detail_list else pd.DataFrame()\n",
    "\n",
    "\n",
    "SensitivityDetailTabs = pd.DataFrame()\n",
    "\n",
    "for idx, values in SensitivityDetailDic.items():\n",
    "    SubTab = pd.DataFrame()\n",
    "    for sub_idx, sub_values in values.items():\n",
    "        sub_values['IscoreType'] = sub_idx\n",
    "        SubTab = pd.concat([SubTab, sub_values])\n",
    "    SubTab['MetricType'] = idx\n",
    "    SensitivityDetailTabs = pd.concat([SensitivityDetailTabs, SubTab])\n",
    "    \n",
    "# =========================\n",
    "# Output Structure Documentation\n",
    "# =========================\n",
    "# AggSensitivityDic:\n",
    "#   Summary of hyperparameter sensitivity using the aggregated metric across all methods/scores.\n",
    "#   - Total_Models: number of unique models accumulated across settings for the hyperparameter.\n",
    "#   - Total_Effective_DataPoints: Total_Models × n_methods × n_metrics (reflects aggregation basis).\n",
    "#   Provides a single consolidated view of sensitivity.\n",
    "\n",
    "# AggSensitivityDetailDic:\n",
    "#   Detailed per-setting statistics using the aggregated metric.\n",
    "#   - n: number of unique models per (Type, Setting) for this specific hyperparameter.\n",
    "#   - n_effective: n × n_methods × n_metrics (effective points behind ISCOREgm).\n",
    "#   - note: human-readable explanation of aggregation basis.\n",
    "#   Shows how each setting behaves for the overall aggregated score.\n",
    "\n",
    "# SensitivityDic[mtype][iscore]:\n",
    "#   Summary sensitivity per method (mtype) and individual score (iscore).\n",
    "\n",
    "# SensitivityDetailDic[mtype][iscore]:\n",
    "#   Per-setting detailed stats per method (mtype) and score (iscore).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gnerating Evaluation Summary Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OnlyMainModels = AnalAccMItableMerged[(AnalAccMItableMerged['Model'].str.contains('SKZFC', na=False)) \n",
    "                                     & (AnalAccMItableMerged['Depth']=='1') \n",
    "                                     & (AnalAccMItableMerged['Comp']=='800')\n",
    "                                     & (AnalAccMItableMerged['LatDim']=='30')]\n",
    "\n",
    "OnlyAblModels = AnalAccMItableMerged[AnalAccMItableMerged['Model'].isin(AblationList)]\n",
    "OnlyAblModels = OnlyAblModels[['MetricType','Model', 'Source', 'Type', 'Depth', 'LatDim', 'Comp', \n",
    "                               'MS', 'MP', 'AC', 'SS', 'RA', 'ISCOREam',\t'ISCOREgm',\t'ISCOREhm']].reset_index(drop=True)\n",
    "\n",
    "# Benchmark comparison table generation\n",
    "BenchCompTabs = pd.concat([OnlyMainModels, BenchAnalAccMItableMerged]).reset_index(drop=True)\n",
    "BenchCompTabs = BenchCompTabs[['MetricType','Model', 'Source', 'Type', 'Depth', 'LatDim', 'Comp', \n",
    "                               'MS', 'MP', 'AC', 'SS', 'RA', 'ISCOREam',\t'ISCOREgm',\t'ISCOREhm']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Ablation study table generation\n",
    "MainForAbl =  BestModels[['MetricType','Model', 'Source', 'Type', 'Depth', 'LatDim', 'Comp', \n",
    "            'MS', 'MP', 'AC', 'SS', 'RA', 'ISCOREam', 'ISCOREgm', 'ISCOREhm']]\n",
    "\n",
    "AblCompTabs = pd.concat([MainForAbl, OnlyAblModels]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Evaluation Summary Tables to 'EvalResults/SummaryTables/' Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BenchCompTabs.to_csv('EvalResults/SummaryTables/BenchCompTabs.csv', index=False) \n",
    "AblCompTabs.to_csv('EvalResults/SummaryTables/AblCompTabs.csv', index=False) \n",
    "AggSensitivityDetailDic.to_csv('EvalResults/SummaryTables/AggSensitivityDetail.csv', index=False) \n",
    "SensitivityDetailTabs.to_csv('EvalResults/SummaryTables/SensitivityDetailTabs.csv', index=False) \n",
    "SenseAccMItable.to_csv('EvalResults/SummaryTables/SenseAccMItable.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
